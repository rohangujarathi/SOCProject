{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec+cnn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Duy0UarBpC65","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"711e280f-2bb4-4d36-eaa4-fa300b4524bf","executionInfo":{"status":"ok","timestamp":1542390773374,"user_tz":300,"elapsed":25076,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount= True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"AXzgxKYFpMmV","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import re\n","import itertools\n","from collections import Counter\n","\n","def load_data_and_labels():\n","  positive_examples = list(open(\"/content/drive/My Drive/Colab Notebooks/positive.csv\").readlines())\n","  positive_examples = [s.strip() for s in positive_examples]\n","  negative_examples = list(open(\"/content/drive/My Drive/Colab Notebooks/negative.csv\").readlines())\n","  negative_examples = [s.strip() for s in negative_examples]\n","  x_text = positive_examples + negative_examples\n","  x_text = [s.split(\" \") for s in x_text]\n","  positive_labels = [[0, 1] for _ in positive_examples]\n","  negative_labels = [[1, 0] for _ in negative_examples]\n","  y = np.concatenate([positive_labels, negative_labels], 0)\n","  return [x_text, y]\n","\n","def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n","  sequence_length = max(len(x) for x in sentences)\n","  padded_sentences = []\n","  for i in range(len(sentences)):\n","      sentence = sentences[i]\n","      num_padding = sequence_length - len(sentence)\n","      new_sentence = sentence + [padding_word] * num_padding\n","      padded_sentences.append(new_sentence)\n","  return padded_sentences\n","\n","def build_vocab(sentences):\n","    # Build vocabulary\n","    word_counts = Counter(itertools.chain(*sentences))\n","    # Mapping from index to word\n","    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n","    # Mapping from word to index\n","    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n","    return [vocabulary, vocabulary_inv]\n","\n","def build_input_data(sentences, labels, vocabulary):\n","    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n","    y = np.array(labels)\n","    return [x, y]\n","  \n","  \n","def load_data():\n","    \"\"\"\n","    Loads and preprocessed data for the MR dataset.\n","    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n","    \"\"\"\n","    # Load and preprocess data\n","    sentences, labels = load_data_and_labels()\n","    sentences_padded = pad_sentences(sentences)\n","    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n","    x, y = build_input_data(sentences_padded, labels, vocabulary)\n","    return [x, y, vocabulary, vocabulary_inv]\n","  \n","x, y, vocabulary, vocabulary_inv_list = load_data()  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"V1f5eqxqrdyg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"3e3d7b70-8bb1-49da-93b4-ff085d1fabcf","executionInfo":{"status":"ok","timestamp":1542392582140,"user_tz":300,"elapsed":383,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}}},"cell_type":"code","source":["print(y)\n","print(x)\n","print(vocabulary[\"a\"])\n","print(vocabulary_inv_list[0])\n","# print(y.argmax(axis=1))\n","# print(y.argmax(axis=0))\n","# print(len(y.argmax(axis=1)))\n","# print(np.random.permutation(np.arange(len(y.argmax(axis=1)))))\n","# np.random.permutation(np.arange(len(y)))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[[0 1]\n"," [0 1]\n"," [0 1]\n"," ...\n"," [1 0]\n"," [1 0]\n"," [1 0]]\n","[[    21  14544      2 ...      0      0      0]\n"," [   755     11    691 ...      0      0      0]\n"," [   134    100    118 ...      0      0      0]\n"," ...\n"," [113186      9      2 ...      0      0      0]\n"," [ 53419      9    133 ...      0      0      0]\n"," [     1   1078   1347 ...      0      0      0]]\n","2\n","<PAD/>\n"],"name":"stdout"}]},{"metadata":{"id":"_RsXUf6UpQjc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"outputId":"afb27168-d02f-4f00-e9e8-5114b1451f31","executionInfo":{"status":"ok","timestamp":1542392590471,"user_tz":300,"elapsed":2570,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}}},"cell_type":"code","source":["!pip install gensim\n","from gensim.models import word2vec\n","from os.path import join, exists, split\n","import os\n","import numpy as np"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n","Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n","Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.46)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.46 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.46)\n","Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.46->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n","Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.46->boto3->smart-open>=1.2.1->gensim) (0.14)\n"],"name":"stdout"}]},{"metadata":{"id":"ROjDuvispUcH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"10e06e97-1455-42a3-abaa-8320656acd0c","executionInfo":{"status":"ok","timestamp":1542392737678,"user_tz":300,"elapsed":141703,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}}},"cell_type":"code","source":["def train_word2vec(sentence_matrix, vocabulary_inv,\n","                   num_features=300, min_word_count=1, context=5):\n","    \n","  # Set values for various parameters\n","  num_workers = 2  # Number of threads to run in parallel\n","  downsampling = 1e-3  # Downsample setting for frequent words\n","\n","  # Initialize and train the model\n","  print('Training Word2Vec model...')\n","  sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n","  embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n","                                      size=num_features, min_count=min_word_count,\n","                                      window=context, sample=downsampling)\n","\n","  # Saving the model for later use. You can load it later using Word2Vec.load()\n","  print('Saving Word2Vec model')\n","  embedding_model.save('/content/drive/My Drive/Colab Notebooks/word2vec_model')\n","\n","  # add unknown words\n","  embedding_weights = {key: embedding_model[word] if word in embedding_model else\n","                            np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n","                       for key, word in vocabulary_inv.items()}\n","  return embedding_weights\n","vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n","w = train_word2vec(x, vocabulary_inv)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Training Word2Vec model...\n","Saving Word2Vec model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"}]},{"metadata":{"id":"hpj3ZQYVxiuY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1472},"outputId":"60af6bfc-10ec-4bf6-bfdf-db6af618edc2","executionInfo":{"status":"error","timestamp":1542393528813,"user_tz":300,"elapsed":16456,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}}},"cell_type":"code","source":["print(w[0])\n","import gensim\n","model12 = gensim.models.word2vec.load('word2vec_model.model')"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[-2.89678723e-01 -1.14690459e+00 -6.47706985e-02 -3.41780841e-01\n","  7.63881300e-03  9.39129770e-01 -3.87457579e-01  4.69469041e-01\n"," -2.20917165e-01 -5.58248758e-01 -6.97229922e-01  2.35928938e-01\n","  8.92669320e-01 -5.03520109e-02 -3.15722883e-01 -1.38160899e-01\n","  3.95923972e-01  3.29567194e-02  3.28095779e-02 -3.95151004e-02\n"," -8.18917930e-01 -1.66836947e-01 -2.46940389e-01  9.48666558e-02\n"," -3.78007721e-03  4.12195057e-01  2.16569766e-01 -3.55429977e-01\n","  4.31115180e-01  2.51411617e-01 -1.10207212e+00  8.13627660e-01\n","  1.45208165e-01  5.16079962e-01 -2.14808613e-01 -5.40146530e-01\n"," -6.13349140e-01  8.42555761e-02 -6.44604536e-03 -2.51312166e-01\n","  5.60393512e-01  2.18838364e-01  2.83518404e-01  4.24545497e-01\n"," -9.42764640e-01 -1.32081628e-01  9.12622213e-01 -8.90382081e-02\n"," -1.12681501e-01  1.88775450e-01  2.10719034e-02 -2.94975013e-01\n","  2.06038684e-01  1.09159485e-01  3.43065709e-01 -6.68582082e-01\n","  1.45096183e-01 -7.31210530e-01  1.41639216e-02 -1.47933528e-01\n"," -3.68248224e-01 -9.16432142e-02  9.00058448e-01  1.62338531e+00\n","  2.44394064e-01  3.02212715e-01  1.37599671e+00  5.31460226e-01\n"," -2.21001595e-01  1.20517775e-01 -1.02344882e+00 -3.00043911e-01\n"," -1.32415891e-01  1.37006521e-01 -1.90076958e-02 -1.38245404e-01\n"," -1.78857014e-01  8.64969730e-01 -4.80621278e-01 -2.90454358e-01\n"," -4.44074839e-01  6.11889362e-01  5.84285319e-01 -9.87292051e-01\n","  1.11694229e+00 -1.36776209e-01  3.13748538e-01 -8.59948099e-01\n"," -4.57529873e-01  2.71861732e-01 -2.32358992e-01 -8.15179586e-01\n","  7.10688531e-02 -3.14994872e-01  2.42900923e-02  7.53061473e-01\n"," -4.15275574e-01 -7.86119759e-01 -9.95780468e-01 -6.78998530e-01\n"," -8.27576876e-01  9.29556668e-01 -4.27622467e-01  3.75280559e-01\n"," -1.34323657e-01 -4.53012705e-01 -6.90334618e-01  2.46232107e-01\n"," -4.84812438e-01  1.37165681e-01 -5.30112267e-01  1.28562891e+00\n"," -9.23509359e-01  6.65177226e-01 -8.16242099e-01 -5.99230111e-01\n"," -5.14084041e-01  7.71485120e-02 -4.08254266e-01 -1.95106015e-01\n"," -1.36208043e-01  4.76728797e-01 -2.98124999e-02  1.10417259e+00\n","  5.49011350e-01 -9.67490226e-02 -6.31471395e-01  7.71320879e-01\n","  2.12125257e-02 -4.14110012e-02  1.12247121e+00  4.92950350e-01\n","  1.82327703e-01 -4.03084040e-01 -5.53027332e-01  4.16966021e-01\n","  2.29039803e-01 -7.29226351e-01 -9.75769386e-02 -3.73887509e-01\n"," -4.05865967e-01  1.04355209e-01  1.02230155e+00 -1.37657374e-01\n","  3.41540426e-01  3.40413123e-01 -2.42274016e-01  4.07750726e-01\n"," -1.08006990e+00  6.29789710e-01  7.06507444e-01 -1.59518905e-02\n","  5.91238499e-01 -1.15067020e-01  3.43066424e-01  5.84765971e-01\n"," -1.19485474e+00  5.54858685e-01  3.53755981e-01  4.24184889e-01\n","  9.09591258e-01 -3.62267345e-01  4.40620452e-01  2.42087126e-01\n"," -8.03966522e-01  5.01462929e-02  6.00931048e-02  7.42071569e-01\n","  5.86259365e-01 -1.33108854e-01  3.00678790e-01 -3.61752331e-01\n"," -7.90464401e-01 -5.14345348e-01 -1.39134256e-02 -5.89855053e-02\n","  6.00344278e-02  3.31094339e-02  1.13370836e+00 -1.33185297e-01\n"," -6.27266109e-01  2.22489417e-01 -3.85546625e-01 -7.06370473e-01\n","  4.12989855e-01  3.32228422e-01  2.43126243e-01  5.36987245e-01\n","  9.22056496e-01 -2.24978447e-01  5.45403779e-01 -2.42938191e-01\n","  4.32977825e-01 -4.47110087e-01 -9.50621217e-02  1.74140617e-01\n","  5.79379275e-02 -2.08203629e-01 -8.72661173e-01  1.31093606e-01\n"," -2.46941119e-01  5.31265616e-01 -1.86611131e-01  6.07716627e-02\n"," -6.24862313e-01  2.49150693e-02 -3.40280145e-01  4.75929886e-01\n"," -4.77616072e-01  8.06757510e-02 -2.48511702e-01 -2.30592057e-01\n","  2.83577979e-01 -6.76314533e-01 -5.61170101e-01  3.54773134e-01\n"," -5.60369968e-01  3.86195540e-01  4.03263539e-01 -4.86266077e-01\n","  3.06861937e-01 -8.11356753e-02 -3.87595519e-02  2.90347397e-01\n"," -1.77266188e-02  6.44462824e-01 -8.53274539e-02  3.09720844e-01\n"," -2.33244970e-01  5.29792011e-01 -5.09343147e-01  8.42024088e-02\n","  3.22580159e-01  2.37734139e-01 -7.53777504e-01 -4.84507859e-01\n","  9.26315010e-01 -8.26082051e-01 -2.95368999e-01 -7.50280842e-02\n"," -7.85072923e-01 -1.58979326e-01 -6.53580725e-02 -6.07011974e-01\n","  1.23034394e+00 -3.97077441e-01  5.37529103e-02 -7.90100992e-01\n","  5.08582115e-01  5.23369014e-01 -2.30642818e-02 -8.54425505e-02\n","  6.66305840e-01  6.41745746e-01 -9.34497416e-02  6.60398304e-01\n","  2.19849929e-01 -1.35785952e-01  3.16885769e-01 -2.74203211e-01\n","  3.88024449e-01 -4.99638408e-01 -3.51514727e-01  5.38161516e-01\n","  3.73535186e-01  3.44199210e-01  2.23356500e-01 -7.88808465e-01\n"," -2.36414135e-01 -6.60654128e-01  3.47068235e-02  1.83404669e-01\n","  1.19214237e+00 -3.13558459e-01  2.59109527e-01 -4.69671190e-01\n"," -4.00774181e-01  5.87986648e-01 -9.98916805e-01  8.73965621e-02\n"," -7.24327028e-01 -7.62377381e-01 -8.91888142e-02  4.78437871e-01\n"," -4.32403505e-01  5.43641388e-01  5.60325503e-01  3.54533136e-01\n","  7.80747592e-01  1.10897899e+00 -6.62866294e-01 -5.62554225e-04\n"," -3.26416969e-01 -5.98238051e-01 -4.31605875e-01 -1.72657177e-01\n"," -2.42423564e-01 -5.70650458e-01 -1.22750089e-01  8.70688200e-01]\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-d66f19af619b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec_model.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'gensim.models.word2vec' has no attribute 'load'"]}]},{"metadata":{"id":"gwj-KlA5peuB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"2b9dcf5d-3edf-4547-8efe-61caa59a942d"},"cell_type":"code","source":["import numpy as np\n","# import data_helpers\n","# from w2v import train_word2vec\n","\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n","from keras.layers.merge import Concatenate\n","# from keras.datasets import imdb\n","from keras.preprocessing import sequence\n","np.random.seed(0)\n","\n","model_type = \"CNN-non-static\"\n","\n","# Model Hyperparameters\n","embedding_dim = 300\n","filter_sizes = (3, 8)\n","num_filters = 10\n","dropout_prob = (0.5, 0.8)\n","hidden_dims = 50\n","\n","# Training parameters\n","batch_size = 64\n","num_epochs = 10\n","\n","# Prepossessing parameters\n","sequence_length = 400\n","max_words = 5000\n","\n","# Word2Vec parameters (see train_word2vec)\n","min_word_count = 1\n","context = 5\n","\n","\n","y = y.argmax(axis=1)\n","shuffle_indices = np.random.permutation(np.arange(len(y)))\n","x = x[shuffle_indices]\n","y = y[shuffle_indices]\n","train_len = int(len(x) * 0.9)\n","x_train = x[:train_len]\n","y_train = y[:train_len]\n","x_test = x[train_len:]\n","y_test = y[train_len:]\n","\n","if sequence_length != x_test.shape[1]:\n","    print(\"Adjusting sequence length for actual size\")\n","    sequence_length = x_test.shape[1]\n","\n","print(\"x_train shape:\", x_train.shape)\n","print(\"x_test shape:\", x_test.shape)\n","print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n","\n","input_shape = (sequence_length,)\n","model_input = Input(shape=input_shape)\n","z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n","z = Dropout(dropout_prob[0])(z)\n","\n","conv_blocks = []\n","for sz in filter_sizes:\n","    conv = Convolution1D(filters=num_filters,kernel_size=sz,padding=\"valid\",activation=\"relu\",strides=1)(z)\n","    conv = MaxPooling1D(pool_size=2)(conv)\n","    conv = Flatten()(conv)\n","    conv_blocks.append(conv)\n","    \n","z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n","\n","z = Dropout(dropout_prob[1])(z)\n","z = Dense(hidden_dims, activation=\"relu\")(z)\n","model_output = Dense(1, activation=\"sigmoid\")(z)\n","\n","model = Model(model_input, model_output)\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","weights = np.array([v for v in w.values()])\n","print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n","embedding_layer = model.get_layer(\"embedding\")\n","embedding_layer.set_weights([weights])\n","    \n","model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n","          validation_data=(x_test, y_test), verbose=2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Adjusting sequence length for actual size\n","x_train shape: (346980, 93)\n","x_test shape: (38554, 93)\n","Vocabulary Size: 113188\n","Initializing embedding layer with word2vec weights, shape (113188, 300)\n","Train on 346980 samples, validate on 38554 samples\n","Epoch 1/10\n"],"name":"stdout"}]}]}