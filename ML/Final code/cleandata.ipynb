{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cleandata.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"aROQq8qSEdms","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DrDb7PMaEjqP","colab_type":"code","outputId":"49bed9dc-4e59-4f15-8aae-82fcadd147ae","executionInfo":{"status":"ok","timestamp":1542390290303,"user_tz":300,"elapsed":69632,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount= True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"le7JvyM4EoZC","colab_type":"code","colab":{}},"cell_type":"code","source":["df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Merged_Dataset.csv', header=None, names = ['Category', 'Data'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1qfuyzN5Esq3","colab_type":"code","colab":{}},"cell_type":"code","source":["# lower case\n","df.Data = df.Data.apply(lambda x: x.lower())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fj5pPcEXEzUq","colab_type":"code","colab":{}},"cell_type":"code","source":["# replacing various categories to Jokes/ Non Jokes\n","\n","df.Category = df['Category'].replace(to_replace={'Proverbs', 'Quotes', 'News' , 'reuters','Wiki'}, value='Non Jokes', regex=True)\n","\n","# df['new'] = df.apply(lambda _: '', axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GyUzKh7LE5FO","colab_type":"code","colab":{}},"cell_type":"code","source":["count = 0\n","for i in range(0,len(df.Data)):\n","  if df.Data[i].startswith('b\"') or df.Data[i].startswith(\"b'\"):\n","    df.Data[i] = df.Data[i][2:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e6mla2xO6PoL","colab_type":"code","outputId":"05534bca-8940-4ef6-a4fe-570804c4ffea","executionInfo":{"status":"ok","timestamp":1542344746101,"user_tz":300,"elapsed":2633,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print(len(df.Data))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["408094\n"],"name":"stdout"}]},{"metadata":{"id":"LPaWrb9kE8fi","colab_type":"code","colab":{}},"cell_type":"code","source":["import string\n","\n","# for i in range(0,len(df.Data)):\n","#     str1 = ''\n","#     for j in df.Data[i]:\n","#         if j not in string.punctuation:\n","#             str1+=j\n","#         df.new[i] = str1\n","# count = 0\n","for i in range(0,len(df.Data)):\n","    x = df.Data[i].translate(None, string.punctuation)\n","    df.Data[i] = x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cxDBYV0NsPqo","colab_type":"code","colab":{}},"cell_type":"code","source":["a = df[df['Data'].map(len)>220] \n","df=df.drop(df.index[df['Data'].map(len)>200])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NjcGhDZ8qZ4k","colab_type":"code","outputId":"04efb41d-3873-4de6-acea-4e253def9b7c","executionInfo":{"status":"ok","timestamp":1542353096122,"user_tz":300,"elapsed":409,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["df.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(383702, 2)"]},"metadata":{"tags":[]},"execution_count":169}]},{"metadata":{"id":"wVPCvwucahMY","colab_type":"code","colab":{}},"cell_type":"code","source":["df.to_csv('/content/drive/My Drive/Colab Notebooks/Merged_Dataset_updated.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rZOtDlNRfW2q","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import re\n","import itertools\n","from collections import Counter\n","\n","def load_data_and_labels():\n","  positive_examples = list(open(\"/content/drive/My Drive/Colab Notebooks/positive.csv\").readlines())\n","  positive_examples = [s.strip() for s in positive_examples]\n","  negative_examples = list(open(\"/content/drive/My Drive/Colab Notebooks/negative.csv\").readlines())\n","  negative_examples = [s.strip() for s in negative_examples]\n","  x_text = positive_examples + negative_examples\n","  x_text = [s.split(\" \") for s in x_text]\n","  positive_labels = [[0, 1] for _ in positive_examples]\n","  negative_labels = [[1, 0] for _ in negative_examples]\n","  y = np.concatenate([positive_labels, negative_labels], 0)\n","  return [x_text, y]\n","\n","def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n","  sequence_length = max(len(x) for x in sentences)\n","  padded_sentences = []\n","  for i in range(len(sentences)):\n","      sentence = sentences[i]\n","      num_padding = sequence_length - len(sentence)\n","      new_sentence = sentence + [padding_word] * num_padding\n","      padded_sentences.append(new_sentence)\n","  return padded_sentences\n","\n","def build_vocab(sentences):\n","    # Build vocabulary\n","    word_counts = Counter(itertools.chain(*sentences))\n","    # Mapping from index to word\n","    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n","    # Mapping from word to index\n","    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n","    return [vocabulary, vocabulary_inv]\n","\n","def build_input_data(sentences, labels, vocabulary):\n","    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n","    y = np.array(labels)\n","    return [x, y]\n","  \n","  \n","def load_data():\n","    \"\"\"\n","    Loads and preprocessed data for the MR dataset.\n","    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n","    \"\"\"\n","    # Load and preprocess data\n","    sentences, labels = load_data_and_labels()\n","    sentences_padded = pad_sentences(sentences)\n","    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n","    x, y = build_input_data(sentences_padded, labels, vocabulary)\n","    return [x, y, vocabulary, vocabulary_inv]\n","  \n","x, y, vocabulary, vocabulary_inv_list = load_data()  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D0cQFG_dkWsU","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install gensim\n","from gensim.models import word2vec\n","from os.path import join, exists, split\n","import os\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pYTnmfN7uNTk","colab_type":"code","outputId":"13edf43c-0451-468e-8514-cc4ac13ff6d2","executionInfo":{"status":"ok","timestamp":1542362679591,"user_tz":300,"elapsed":179376,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"cell_type":"code","source":["def train_word2vec(sentence_matrix, vocabulary_inv,\n","                   num_features=300, min_word_count=1, context=5):\n","    \n","  # Set values for various parameters\n","  num_workers = 2  # Number of threads to run in parallel\n","  downsampling = 1e-3  # Downsample setting for frequent words\n","\n","  # Initialize and train the model\n","  print('Training Word2Vec model...')\n","  sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n","  embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n","                                      size=num_features, min_count=min_word_count,\n","                                      window=context, sample=downsampling)\n","\n","  # Saving the model for later use. You can load it later using Word2Vec.load()\n","  print('Saving Word2Vec model')\n","  embedding_model.save('word2vec_model')\n","\n","  # add unknown words\n","  embedding_weights = {key: embedding_model[word] if word in embedding_model else\n","                            np.random.uniform(-0.25, 0.25, embedding_model.vector_size)\n","                       for key, word in vocabulary_inv.items()}\n","  return embedding_weights\n","vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n","w = train_word2vec(x, vocabulary_inv)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training Word2Vec model...\n","Saving Word2Vec model\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n","/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"],"name":"stderr"}]},{"metadata":{"id":"OH6OqA3OAx1K","colab_type":"code","outputId":"c8bc4b95-829c-4297-c260-d080596535d8","executionInfo":{"status":"error","timestamp":1542363325212,"user_tz":300,"elapsed":402,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"cell_type":"code","source":["print(len(y))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-21-661a78065664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"]}]},{"metadata":{"id":"6QHh37Ve1Ft_","colab_type":"code","outputId":"d7d498f3-0143-498f-f637-a583eab9ec65","executionInfo":{"status":"error","timestamp":1542363197857,"user_tz":300,"elapsed":393,"user":{"displayName":"Rohan Gujarathi","photoUrl":"","userId":"12179700570471465676"}},"colab":{"base_uri":"https://localhost:8080/","height":231}},"cell_type":"code","source":["import numpy as np\n","# import data_helpers\n","# from w2v import train_word2vec\n","\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n","from keras.layers.merge import Concatenate\n","# from keras.datasets import imdb\n","from keras.preprocessing import sequence\n","np.random.seed(0)\n","\n","model_type = \"CNN-non-static\"\n","\n","# Model Hyperparameters\n","embedding_dim = 300\n","filter_sizes = (3, 8)\n","num_filters = 10\n","dropout_prob = (0.5, 0.8)\n","hidden_dims = 50\n","\n","# Training parameters\n","batch_size = 64\n","num_epochs = 10\n","\n","# Prepossessing parameters\n","sequence_length = 400\n","max_words = 5000\n","\n","# Word2Vec parameters (see train_word2vec)\n","min_word_count = 1\n","context = 5\n","\n","\n","y = y.argmax(axis=0)\n","shuffle_indices = np.random.permutation(np.arange(len(y)))\n","x = x[shuffle_indices]\n","y = y[shuffle_indices]\n","train_len = int(len(x) * 0.9)\n","x_train = x[:train_len]\n","y_train = y[:train_len]\n","x_test = x[train_len:]\n","y_test = y[train_len:]\n","\n","if sequence_length != x_test.shape[1]:\n","    print(\"Adjusting sequence length for actual size\")\n","    sequence_length = x_test.shape[1]\n","\n","print(\"x_train shape:\", x_train.shape)\n","print(\"x_test shape:\", x_test.shape)\n","print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n","\n","input_shape = (sequence_length,)\n","model_input = Input(shape=input_shape)\n","z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n","z = Dropout(dropout_prob[0])(z)\n","\n","conv_blocks = []\n","for sz in filter_sizes:\n","    conv = Convolution1D(filters=num_filters,kernel_size=sz,padding=\"valid\",activation=\"relu\",strides=1)(z)\n","    conv = MaxPooling1D(pool_size=2)(conv)\n","    conv = Flatten()(conv)\n","    conv_blocks.append(conv)\n","    \n","z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n","\n","z = Dropout(dropout_prob[1])(z)\n","z = Dense(hidden_dims, activation=\"relu\")(z)\n","model_output = Dense(1, activation=\"sigmoid\")(z)\n","\n","model = Model(model_input, model_output)\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","weights = np.array([v for v in w.values()])\n","print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n","embedding_layer = model.get_layer(\"embedding\")\n","embedding_layer.set_weights([weights])\n","    \n","model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n","          validation_data=(x_test, y_test), verbose=2)  \n","\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-15-6612127f6b85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mshuffle_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"]}]}]}